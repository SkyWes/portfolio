[
  {
    "objectID": "posts/ndvi-vs-income/index.html",
    "href": "posts/ndvi-vs-income/index.html",
    "title": "Vegitation Density in the Greater Vancouver Area",
    "section": "",
    "text": "I used satellite data to analyze vegetation density and compared it to Stats Canada income data using Python and Power BI.\n\nDownload the python notebook\n\n\nKey Questions\nThe urban heat island (UHI) effect results in the warming of cities due to reduced vegetation, with neighborhoods lacking green space being most vulnerable to this effect. This analysis investigates:\n\nWhich neighborhoods in Vancouver are the greenest?\nWhat is the relationship between income and vegetation density in the Greater Vancouver Area?\n\n\n\n\nData Sources and Tools\nTo explore these questions, I used: - Sentinel-2 satellite data to analyze vegetation density. - Statistics Canada census data to examine income distribution. - Python for data processing and transformation. - Power BI for creating an interactive data visualization dashboard.\n\n\n\nData Transformation Steps\n\n1. Importing and Merging Satellite Data\nTwo TIFF files covered the geographical area of interest. These were imported as shown below:\n\n\n\n\n\n\n\n\nNDVI Image 1\n\n\n\n\n\n\n\nNDVI Image 2\n\n\n\n\n\n\nFigure 1\n\n\n\nAfter import, the two images were merged to form a single comprehensive NDVI image of the area.\n\n\n\nMerged NDVI\n\n\n\n\n2. Masking Water Areas\nTo calculate accurate vegetation density statistics, I created a mask to exclude water data:\n\n\n\nMasked NDVI\n\n\n\n\n3. Calculating and Visualizing NDVI\nThe Normalized Difference Vegetation Index (NDVI), a measure of vegetation health, was computed for the area. A histogram was plotted to visualize the distribution of NDVI values:\n\n\n\nNDVI Distribution\n\n\n\n\n4. Income Data Transformation\nAfter importing and cleaning income data from Statistics Canada, I visualized the distribution of average income for the Greater Vancouver Area:\n\n\n\nAverage Income Distribution\n\n\n\n\n5. Building the Interactive Dashboard\nThe cleaned datasets were imported into Power BI, where I built a semantic model and designed an interactive dashboard. Explore the dashboard here:\n\n\n\nClick img to use Dashboard\n\n\n\n\n\n\nKey Findings\n\nIncome and Vegetation Density: There is a weak positive correlation between income and vegetation density, likely influenced by higher-income individuals having the flexibility to acquire properties with more green space.\nNotable Neighborhoods:\n\nOlympic Village: A high-income area with lower NDVI values, indicating less greenery.\nUniversity of British Columbia (UBC): Despite its relatively lower income (likely due to a large student population), UBC has high NDVI values, showcasing significant greenery.\n\n\nThese findings highlight potential areas for targeted urban greening initiatives to mitigate the urban heat island effect and promote environmental equity."
  },
  {
    "objectID": "posts/ev-sales/index.html#setting-up-the-r-environment",
    "href": "posts/ev-sales/index.html#setting-up-the-r-environment",
    "title": "Canada’s Path to an EV Future",
    "section": "Setting up the R environment",
    "text": "Setting up the R environment\nThe following libraries will allow us to import, tidy, and visualize the data.\n\n\nCode\n# to style code according to the tidyverse style guide\n# install.packages(\"styler\")\n\nlibrary(knitr) # for rendering this document\nopts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE)\noptions(width = 100, dplyr.width = 100)\n\nlibrary(tidyverse) # for cleaning the data\nlibrary(lubridate) # date handling\nlibrary(forcats) # factor manipulation\nlibrary(janitor) # tidying variables\nlibrary(kableExtra) # creating data tables"
  },
  {
    "objectID": "posts/ev-sales/index.html#importing-and-validating-the-data",
    "href": "posts/ev-sales/index.html#importing-and-validating-the-data",
    "title": "Canada’s Path to an EV Future",
    "section": "Importing and validating the data",
    "text": "Importing and validating the data\nThere are two Statistics Canada datasets for new motor vehicle registrations (NMVR) with breakdowns by fuel type, geography, and vehicle type. One published in 2021 which is yearly data, and the most recent quarterly data up to and including Q2 2024. The global data is a single csv with car sales with electric and non-electric values.\n\n\nCode\n# output hidden\n\n# reading in the Stats Canada datasets.\n\n# https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=2010002101\n# citation: Statistics Canada. Table 20-10-0021-01  New motor vehicle registrations, inactive\nstatscan2021 &lt;- read_csv(\"data/statscan_2021.csv\")\n\n# https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=2010002401\n# citation: Statistics Canada. Table 20-10-0024-01  New motor vehicle registrations, quarterly\nstatscan2024 &lt;- read_csv(\"data/statscan_2024.csv\")\n\n# now for the global data\n# https://ourworldindata.org/electric-car-sales\n# citation: International Energy Agency. Global EV Outlook 2024. – processed by Our World in Data\nstatsglobal_csv &lt;- read_csv(\"data/car-sales.csv\")\n\n\n\nThis code is inspecting the structure of the data to see how we should proceed.\n\n\nCode\n# output hidden\n\n# check structure\nstr(statscan2021)\nhead(statscan2021)\n\nstr(statscan2024)\nhead(statscan2024)\n\nstr(statsglobal_csv)\nsummary(statsglobal_csv)\n\nstatsglobal_csv |&gt;\n  select(Entity) |&gt;\n  distinct() # Important: this reveals a 'World' geography with totals.\n\n\n\nBoth Stats Canada datasets have the same structure except for granularity and the ref_date column. Before joining the data, the date columns must be made compatible.\nThe summary function can be used to validate the date and value ranges, to check for NA or missing data, and for comparing the categories.\n\n\nCode\n# The 2021 data only has years of class dbl while the 2024 data has yyyy-mm as class chr\n# validate date range and distinct categories.\nstatscan2021 |&gt;\n  # we are only interested in these variables\n  select(REF_DATE, GEO, `Fuel type`, `Vehicle type`) |&gt;\n  mutate(\n    geo = as.factor(GEO), # create factors for the summary function\n    fuel_type = as.factor(`Fuel type`),\n    vehicle_type = as.factor(`Vehicle type`),\n    .keep = \"unused\" # don't include referenced columns\n  ) |&gt;\n  summary()\n\n\n    REF_DATE                                      geo                         fuel_type  \n Min.   :2011   Alberta                             : 385   All fuel types         :605  \n 1st Qu.:2013   British Columbia and the Territories: 385   Battery electric       :605  \n Median :2016   Canada                              : 385   Diesel                 :605  \n Mean   :2016   Manitoba                            : 385   Gasoline               :605  \n 3rd Qu.:2019   New Brunswick                       : 385   Hybrid electric        :605  \n Max.   :2021   Newfoundland and Labrador           : 385   Other fuel types       :605  \n                (Other)                             :1925   Plug-in hybrid electric:605  \n                 vehicle_type\n Multi-purpose vehicles:847  \n Passenger cars        :847  \n Pickup trucks         :847  \n Total, vehicle type   :847  \n Vans                  :847  \n                             \n                             \n\n\nCode\nstatscan2024 |&gt;\n  select(REF_DATE, GEO, `Fuel type`, `Vehicle type`) |&gt;\n  mutate(\n    ref_date = ym(REF_DATE), # convert to date for the summary function\n    geo = as.factor(GEO),\n    fuel_type = as.factor(`Fuel type`),\n    vehicle_type = as.factor(`Vehicle type`),\n    .keep = \"unused\"\n  ) |&gt;\n  summary()\n\n\n    ref_date                                            geo                         fuel_type   \n Min.   :2017-01-01   Alberta                             :1050   All fuel types         :1650  \n 1st Qu.:2018-10-01   British Columbia and the Territories:1050   Battery electric       :1650  \n Median :2020-08-16   Canada                              :1050   Diesel                 :1650  \n Mean   :2020-08-15   Manitoba                            :1050   Gasoline               :1650  \n 3rd Qu.:2022-07-01   New Brunswick                       :1050   Hybrid electric        :1650  \n Max.   :2024-04-01   Newfoundland and Labrador           :1050   Other fuel types       :1650  \n                      (Other)                             :5250   Plug-in hybrid electric:1650  \n                 vehicle_type \n Multi-purpose vehicles:2310  \n Passenger cars        :2310  \n Pickup trucks         :2310  \n Total, vehicle type   :2310  \n Vans                  :2310  \n                              \n                              \n\n\n\nWe can see from the summaries that the vehicle types and fuel types of both datasets are equal but not all of the distinct geographies are listed. Let’s check and compare the distinct geographies to make sure joining the data goes smoothly.\n\n\nCode\n# checking all of the distinct geographies in the dataset\nstats1 &lt;- statscan2021 |&gt;\n  select(GEO) |&gt;\n  distinct()\n\nstats2 &lt;- statscan2024 |&gt;\n  select(GEO) |&gt;\n  distinct()\n\nall.equal(stats1, stats2)\n\n\n[1] TRUE"
  },
  {
    "objectID": "posts/ev-sales/index.html#tidying-the-data",
    "href": "posts/ev-sales/index.html#tidying-the-data",
    "title": "Canada’s Path to an EV Future",
    "section": "Tidying the data",
    "text": "Tidying the data\nBefore joining the data ensure all variables are shared by creating a ‘year’, ‘quarter’, and ‘ref_date’ variable where needed.\n\n\nCode\nstatscan2021_clean &lt;- statscan2021 |&gt;\n  clean_names(\"snake\") |&gt;\n  \n  # Only keep needed variables\n  select(ref_date, geo, fuel_type, vehicle_type, value) |&gt;\n  \n  # create a separate year column\n  rename(year = ref_date) |&gt;\n  \n  # and ref_date and quarter columns to properly union the two datasets\n  # Note: any quarterly analysis may not work as expected without first\n  # filtering out the NAs removing years without quarterly data\n  mutate(\n    ref_date = NA,\n    quarter = NA\n  )\n\n\nstatscan2024_clean &lt;- statscan2024 |&gt;\n  clean_names(\"snake\") |&gt;\n  \n  # Only keep needed variables\n  select(ref_date, geo, fuel_type, vehicle_type, value) |&gt;\n  \n  # create year, quarter columns and mutate ref_date to class 'date'.\n  mutate(\n    quarter = quarter(ym(ref_date)),\n    ref_date = ym(ref_date),\n    year = year(ref_date)\n  )\n\n\n\nIn the meta-data, it states that, “Data for Newfoundland and Labrador, Nova Scotia and Alberta are currently not available…” This code checks to ensure these provinces don’t have any data.\n\nCode\n# create object with NA geographies\n\nna_geo &lt;- c(\"Newfoundland and Labrador\", \"Alberta\", \"Nova Scotia\")\n\n# check if there are only NAs for those provinces before dropping rows.\nstatscan2021_clean |&gt;\n  filter(geo %in% na_geo, is.na(value) == FALSE)\nstatscan2024_clean |&gt;\n  filter(geo %in% na_geo, is.na(value) == FALSE)\n\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\nZero rows output confirms we can safely remove observations for those provinces.\n\n\nCode\n# drop the observations with provinces in 'na_geo'\nstatscan2021_clean &lt;- statscan2021_clean |&gt;\n  filter(!(geo %in% na_geo))\n\nstatscan2024_clean &lt;- statscan2024_clean |&gt;\n  filter(!(geo %in% na_geo))\n\n\n\nTo join the data, filter out the overlapping years in favour of the higher granularity in the quarterly data set, then the observations can be stacked with a union.\n\n\nCode\n# only include years prior to 2017\nstatscan2021_clean &lt;- statscan2021_clean |&gt;\n  filter(year &lt; \"2017\")\n\n# union the datasets and check the result\n(statscan_nmvr &lt;- statscan2024_clean |&gt;\n  union(statscan2021_clean) |&gt;\n    \n  # we won't need vehicle types\n  filter(vehicle_type == \"Total, vehicle type\") |&gt;\n  select(-vehicle_type) |&gt;\n    \n  # adding in superscript for a table footnote\n  mutate(\n    fuel_type = case_when(str_detect(fuel_type, \"Other fuel types\") ~ \"Other fuel types\\u00B2\",\n      .default = fuel_type\n    ),\n\n    # create new column to group fuel types into either EV or non-EV\n    ev_fuel = case_when(fuel_type %in% c(\"Battery electric\", \"Plug-in hybrid electric\") ~ \"EV\",\n      !(fuel_type %in% c(\"All fuel types\", \"Battery electric\", \"Plug-in hybrid electric\")) ~ \"Non-EV\",\n      .default = fuel_type\n    )\n  )\n)"
  },
  {
    "objectID": "posts/ev-sales/index.html#preparing-the-data",
    "href": "posts/ev-sales/index.html#preparing-the-data",
    "title": "Canada’s Path to an EV Future",
    "section": "Preparing the data",
    "text": "Preparing the data\n1. Table data frame\nNow the data is ready to be prepared for visualizing by creating some data frames starting with a table.\n\n\nCode\n####################################################\n#########  Kable Table ############################\n\n# for easily referencing BC\nbc_geo &lt;- \"British Columbia and the Territories\"\n\n# preparing data for a table\nstatscan_nmvr_kable &lt;- statscan_nmvr |&gt;\n  \n  # keep only Canada and BC. Take a snapshot\n  # of the past five and a half years (2024 is up to Q2) only.\n  filter(\n    geo %in% c(\"Canada\", bc_geo),\n    year &gt;= 2019\n  ) |&gt;\n  group_by(geo, fuel_type, year) |&gt;\n  summarise(units = sum(value)) |&gt;\n  \n  # pivoting to a wide format for the table\n  pivot_wider(names_from = year, values_from = units) |&gt;\n  arrange(desc(geo), desc(`2023`)) |&gt;\n  ungroup()\n\n# custom column headers for the table\nmy_col_names &lt;- c(\" \", \" \", \" \", \"Units\", \" \", \" \", \" \")\nnew_headers &lt;- c(\"Fuel Type\" = 1, \"2019\" = 1, \"2020\" = 1, \"2021\" = 1, \"2022\" = 1, \"2023\" = 1, \"2024\" = 1)\n\n\n2. Column chart data frame\nThe following code is for a column chart with factors and labels for highlighting specific data.\n\n\nCode\n####################################################\n#########  Column chart ###########################\n\n# data frame for column chart\nnmvr_province &lt;- statscan_nmvr |&gt;\n  filter(\n    ev_fuel != \"All fuel types\", # Important: this is a totals row\n    geo != \"Canada\",\n    year == 2023\n  ) |&gt;\n  group_by(geo, ev_fuel) |&gt;\n  summarise(nmvr = sum(value)) |&gt;\n  mutate(share = nmvr / sum(nmvr) * 100)\n\n# reordering factor levels\nnmvr_province_lvls &lt;- nmvr_province |&gt;\n  filter(ev_fuel == \"EV\") |&gt;\n  arrange(share) |&gt;\n  pull(geo)\n\nnmvr_province &lt;- mutate(nmvr_province,\n  geo = factor(geo, levels = nmvr_province_lvls)\n)\n\n# creating df for highlighting and labeling in plots\nnmvr_bc &lt;- nmvr_province |&gt;\n  filter(geo == bc_geo) |&gt;\n  mutate(label = ifelse(ev_fuel == \"EV\", round(share / 100, 2), NA))\n\n# bc labels\nnmvr_bc_ev &lt;- nmvr_bc |&gt;\n  filter(ev_fuel == \"EV\") |&gt;\n  pull(label)\n\n# ev levels\nnmvr_ev_lvls &lt;- nmvr_province |&gt;\n  filter(ev_fuel == \"EV\") |&gt;\n  arrange(nmvr) |&gt;\n  pull(geo)\n\n# ev labels\nnmvr_ev_labs &lt;- nmvr_province |&gt;\n  filter(ev_fuel == \"EV\") |&gt;\n  arrange(desc(nmvr)) |&gt;\n  head(n = 3)\n\n\n3. Timeseries data frames\nData frames for quarterly and yearly plots.\n\n\nCode\n####################################################\n#########  Timeseries #############################\n\n# function for common share transformation\nshare_calc &lt;- function(df, g, scale) {\n  return(df |&gt;\n    filter(geo == g) |&gt;\n    group_by({{ scale }}, ev_fuel) |&gt;\n    summarise(nmvr = sum(value)) |&gt;\n    mutate(share = nmvr / sum(nmvr)) |&gt;\n    filter(ev_fuel == \"EV\"))\n}\n\n# data frame for quarterly timeseries\nnmvr_quarterly_share &lt;- statscan_nmvr |&gt;\n  filter(\n    fuel_type != \"All fuel types\",\n    # only the quarterly data\n    !(is.na(quarter))\n  )\n\n# one for BC\nnmvr_quarterly_share_bc &lt;- share_calc(nmvr_quarterly_share, bc_geo, ref_date)\n\n# one for Canada\nnmvr_quarterly_share_can &lt;- share_calc(nmvr_quarterly_share, \"Canada\", ref_date)\n\n# finding the year over year rate of EV share growth\nnmvr_share_rate &lt;- share_calc(\n  filter(\n    statscan_nmvr,\n    fuel_type != \"All fuel types\",\n    year != 2024\n  ),\n  \"Canada\", year\n) |&gt;\n  arrange(year) |&gt;\n  ungroup() |&gt;\n  mutate(share_growth = (share - lag(share)) / lag(share) * 100)\n\n\n4. Global data\nThe following code prepares data frames for plotting and statistics of the global data.\n\n\nCode\n####################################################\n#########  Global Data ############################\n\n# tidying global data\nstatsglobal_clean &lt;- statsglobal_csv |&gt;\n  select(-Code) |&gt;\n  clean_names(\"snake\") |&gt;\n  mutate(\n    electric_cars = electric_cars_sold,\n    non_electric_cars = round(non_electric_car_sales),\n    ev_yearly_share = electric_cars / (electric_cars + non_electric_cars),\n  ) |&gt;\n  pivot_longer(cols = ends_with(\"cars\"), names_to = \"type\", values_to = \"sales\")\n\n# ordering global data for factor levels\ntop_ev_share_global_levels &lt;- statsglobal_clean |&gt;\n  filter(year == max(year)) |&gt;\n  summarize(.by = c(entity, year), ev_share = max(ev_yearly_share)) |&gt;\n  arrange(desc(ev_share)) |&gt;\n  select(entity) |&gt;\n  pull(entity)\n\n\n# Creating data frame and factor levels for global share plot\nev_share_global &lt;- statsglobal_clean |&gt;\n  mutate(entity = factor(entity, levels = top_ev_share_global_levels)) |&gt;\n  filter(as.integer(entity) &lt; 6 | entity == \"Canada\") |&gt;\n  group_by(entity, year) |&gt;\n  \n  # we take the max and not the sum of ev_yearly_share here because the share\n  # was calculated before pivoted, creating repeated values for each 'type'\n  summarise(ev_share = max(ev_yearly_share))\n\n# finding the max value for labelling\nmax_share &lt;- ev_share_global |&gt;\n  group_by(entity) |&gt;\n  summarise(share = max(ev_share))\n\n# creating new column for highlighting Canada\nev_share_global_fill &lt;- ev_share_global |&gt;\n  mutate(is_canada = as.character(entity == \"Canada\"))\n\n####################################################\n# creating data frame for share growth stats\nglobal_share_growth &lt;- statsglobal_clean |&gt;\n  filter(\n    entity == \"World\",\n    type == \"electric_cars\"\n  ) |&gt;\n  arrange(year) |&gt;\n  mutate(share_growth = (ev_yearly_share - lag(ev_yearly_share)) / lag(ev_yearly_share) * 100)\n\n\n5. Theme and palette\nTo maintain a consistent style, we can prepare a theme to style the plots and define an accessible colour palette.\n\n\nCode\n# output: false\n# create a ggplot custom theme\n\ntheme &lt;- theme_minimal() +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 3, b = 3)),\n    axis.title.y = element_blank(),\n    strip.background = element_blank(),\n    axis.line.x = element_line(color = \"gray\"),\n    axis.line.y = element_line(color = \"gray\"),\n    axis.ticks.x = element_line(color = \"gray\"),\n    legend.title = element_blank(),\n    legend.position = \"top\",\n    legend.key.size = unit(4, \"mm\"),\n    legend.justification.top = -0.5,\n    legend.text.position = \"left\",\n    legend.box.margin = margin(t = 0, b = 0, unit = \"pt\"),\n    title = element_text(hjust = 1),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    plot.margin = margin(10, 10, 10, 10),\n    plot.caption = element_text(hjust = 0.1, vjust = 0.1),\n  )\n\ntheme_set(theme)\n\n# create accessible colour palette\nclr_plt &lt;- hcl.colors(5, palette = \"blue-red\")\n# \"#023FA5\" \"#A1A6C8\" \"#E2E2E2\" \"#CA9CA4\" \"#8E063B\""
  },
  {
    "objectID": "posts/ev-sales/index.html#understanding-the-data",
    "href": "posts/ev-sales/index.html#understanding-the-data",
    "title": "Canada’s Path to an EV Future",
    "section": "Understanding the data",
    "text": "Understanding the data\nIn BC there were 35,887 battery electric NMVR in the year of 2023, more than double from 12,288 in 2019, only 4 years prior. Canada as a whole saw an even greater increase in battery electric NMVR over the past 4 years quadrupling from 35,523 in 2019 to 143,661 in 2023. Keep in mind, 2024 data only includes Q1 and Q2, suggesting we are well on the way to once again breaking last year’s EV registration record.\n\nCode\nstatscan_nmvr_kable &lt;- statscan_nmvr_kable |&gt;\n  \n  # these mutate calls are for highlighting the specific values in the table\n  # using cell_spec()\n  mutate(\n    `2023` = cell_spec(scales::comma(`2023`), \"html\",\n      color = case_when(`2023` == 35887 ~ clr_plt[1],\n        `2023` == 143661 ~ clr_plt[5],\n        .default = \"black\"\n      ),\n      bold = case_when(`2023` == 35887 | `2023` == 143661 ~ TRUE,\n        .default = FALSE\n      )\n    )\n  ) |&gt;\n  mutate(\n    `2019` = cell_spec(scales::comma(`2019`), \"html\",\n      color = case_when(`2019` == 12288 ~ clr_plt[1],\n        `2019` == 35523 ~ clr_plt[5],\n        .default = \"black\"\n      ),\n      bold = case_when(`2019` == 12288 | `2019` == 35523 ~ TRUE,\n        .default = FALSE\n      )\n    )\n  )\n\n# building a table for BC\nstatscan_nmvr_kable |&gt;\n  filter(geo == bc_geo) |&gt;\n  select(-geo) |&gt;\n  kbl(\n    escape = FALSE, \"html\",\n\n    # we are using custom column names\n    col.names = my_col_names,\n    format.args = list(big.mark = \",\"), table.attr = \"quarto-disable-processing=true\", # disable quarto processing to avert bug that ignores styling\n    align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\")\n  ) |&gt;\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\"), html_font = \"helvetica\") |&gt;\n  column_spec(1, width_min = \"11em\", bold = TRUE, border_right = TRUE) |&gt;\n  row_spec(0, bold = TRUE, background = clr_plt[3], align = \"r\") |&gt;\n  \n  # highlight the EV rows\n  row_spec(c(3), background = clr_plt[3]) |&gt;\n  \n  # adding more custom headers\n  add_header_above(new_headers, background = clr_plt[3], bold = TRUE, align = list(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\")) |&gt;\n  add_header_above(c(\"Statistics\" = 1, \"Number of vehicles\" = 6),\n    background = clr_plt[3],\n    bold = TRUE, align = list(\"r\", \"c\")\n  ) |&gt;\n  add_header_above(c(\"Vehicle type\" = 1, \"Total, vehicle type\\u00B9\" = 6),\n    background = clr_plt[3],\n    bold = TRUE, align = list(\"r\", \"c\")\n  ) |&gt;\n  add_header_above(c(\"Geography\" = 1, \"British Columbia and the Territories\" = 6),\n    background = clr_plt[3],\n    bold = TRUE, align = list(\"r\", \"c\")\n  ) |&gt;\n  footnote(number = c(\"Total vehicle type excludes buses, trailers, recreational vehicles, motorcycles, snowmobiles, golf carts, etc.\", \"Other fuel types include liquid propane, natural gas, hydrogen, etc.\"))\n# building a table for Canada\nstatscan_nmvr_kable |&gt;\n  filter(geo == \"Canada\") |&gt;\n  select(-geo) |&gt;\n  kbl(\n    escape = FALSE, \"html\",\n\n    # we are using custom column names\n    col.names = my_col_names,\n    format.args = list(big.mark = \",\"), table.attr = \"quarto-disable-processing=true\", # disable quarto processing to avert bug that ignores styling\n    align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\")\n  ) |&gt;\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\"), html_font = \"helvetica\") |&gt;\n  column_spec(1, width_min = \"11em\", bold = TRUE, border_right = TRUE) |&gt;\n  row_spec(0, bold = TRUE, background = clr_plt[3], align = \"r\") |&gt;\n  \n  # highlight the EV rows\n  row_spec(c(3), background = clr_plt[3]) |&gt;\n  \n  # adding more custom headers\n  add_header_above(new_headers, background = clr_plt[3], bold = TRUE, align = list(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\")) |&gt;\n  add_header_above(c(\"Statistics\" = 1, \"Number of vehicles\" = 6),\n    background = clr_plt[3],\n    bold = TRUE, align = list(\"r\", \"c\")\n  ) |&gt;\n  add_header_above(c(\"Vehicle type\" = 1, \"Total, vehicle type\\u00B9\" = 6),\n    background = clr_plt[3],\n    bold = TRUE, align = list(\"r\", \"c\")\n  ) |&gt;\n  add_header_above(c(\"Geography\" = 1, \"Canada\" = 6),\n    background = clr_plt[3],\n    bold = TRUE, align = list(\"r\", \"c\")\n  ) |&gt;\n  footnote(number = c(\"Total vehicle type excludes buses, trailers, recreational vehicles, motorcycles, snowmobiles, golf carts, etc.\", \"Other fuel types include liquid propane, natural gas, hydrogen, etc.\"))\n\n\n\n\n\n \n\nGeography\nBritish Columbia and the Territories\n\n\nVehicle type\nTotal, vehicle type¹\n\n\nStatistics\nNumber of vehicles\n\n\nFuel Type\n2019\n2020\n2021\n2022\n2023\n2024\n\n  \n      \n      \n      \n    Units \n      \n      \n      \n  \n \n\n  \n    All fuel types \n    218,001 \n    181,183 \n    204,892 \n    183,277 \n    210,669 \n    111,492 \n  \n  \n    Gasoline \n    181,447 \n    144,587 \n    155,742 \n    129,999 \n    136,575 \n    72,339 \n  \n  \n    Battery electric \n    12,288 \n    12,087 \n    18,135 \n    24,832 \n    35,887 \n    17,003 \n  \n  \n    Hybrid electric \n    8,598 \n    8,715 \n    13,936 \n    13,174 \n    19,844 \n    11,164 \n  \n  \n    Diesel \n    10,940 \n    12,640 \n    11,296 \n    10,392 \n    10,597 \n    5,670 \n  \n  \n    Plug-in hybrid electric \n    4,692 \n    3,124 \n    5,783 \n    4,866 \n    7,751 \n    5,303 \n  \n  \n    Other fuel types² \n    36 \n    30 \n    0 \n    14 \n    15 \n    13 \n  \n\n\n\n1 Total vehicle type excludes buses, trailers, recreational vehicles, motorcycles, snowmobiles, golf carts, etc.\n\n2 Other fuel types include liquid propane, natural gas, hydrogen, etc.\n\n\n\n\n\n\n\n\n \n\nGeography\nCanada\n\n\nVehicle type\nTotal, vehicle type¹\n\n\nStatistics\nNumber of vehicles\n\n\nFuel Type\n2019\n2020\n2021\n2022\n2023\n2024\n\n  \n      \n      \n      \n    Units \n      \n      \n      \n  \n \n\n  \n    All fuel types \n    1,930,445 \n    1,545,561 \n    1,646,604 \n    1,513,104 \n    1,714,913 \n    926,049 \n  \n  \n    Gasoline \n    1,776,571 \n    1,384,928 \n    1,415,128 \n    1,233,180 \n    1,316,444 \n    697,123 \n  \n  \n    Battery electric \n    35,523 \n    39,036 \n    58,952 \n    98,620 \n    143,661 \n    84,260 \n  \n  \n    Hybrid electric \n    38,390 \n    41,453 \n    79,328 \n    81,049 \n    135,682 \n    75,873 \n  \n  \n    Diesel \n    59,089 \n    64,769 \n    65,876 \n    75,247 \n    74,020 \n    38,896 \n  \n  \n    Plug-in hybrid electric \n    20,642 \n    15,317 \n    27,315 \n    24,990 \n    45,090 \n    29,884 \n  \n  \n    Other fuel types² \n    230 \n    58 \n    5 \n    18 \n    16 \n    13 \n  \n\n\n\n1 Total vehicle type excludes buses, trailers, recreational vehicles, motorcycles, snowmobiles, golf carts, etc.\n\n2 Other fuel types include liquid propane, natural gas, hydrogen, etc.\n\n\n\n\n\n\n\n\nThe provinces\nHow does BC compare to other provinces though? Which province has the highest share of EV NMVR for 2023?\n\n\nCode\ntheme_set(theme)\n\nnmvr_province |&gt;\n  # to place the ev share against the y axis subtract 'share' from 100\n  ggplot(aes(x = geo, y = 100 - share, label = 100 - share, fill = ev_fuel)) +\n\n  # use position \"fill\" to represent stacked columns as proportion of 100%\n  geom_col(position = \"fill\") +\n\n  # manually fill colour values\n  scale_fill_manual(\n    values = c(clr_plt[3], clr_plt[2]),\n    labels = c(\"Non-EV\", \"EV\")\n  ) +\n\n  # change axis labels to represent percentages\n  scale_y_continuous(labels = scales::label_percent()) +\n\n  # add second geom layer to highlight BC\n  geom_col(\n    data = nmvr_bc, aes(\n      x = geo, y = share,\n      fill = ev_fuel\n    ),\n    position = \"fill\",\n    fill = c(clr_plt[1], clr_plt[3])\n  ) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Share of new vehicle registerations that are electric, by Province, 2023\",\n    subtitle = \"Includes fully battery-electric and plug-in hybrids.\",\n    caption = \"Data source: Statistics Canada. Tables 20-10-0021-01 and 20-10-0024-01, New motor vehicle registrations\"\n  ) +\n  geom_label(\n    data = nmvr_bc, aes(x = geo, y = label, label = str_c(as.character(round(share, 1)), \"% of all BC NMVR were EV in 2023\")),\n    show.legend = F, na.rm = T, nudge_y = 0.31, fill = \"white\", size = 3\n  ) +\n  geom_segment(aes(x = bc_geo, y = nmvr_bc_ev - 0.01, yend = nmvr_bc_ev + 0.04),\n    data = nmvr_bc, arrow = arrow(length = unit(0.15, \"cm\")),\n    color = \"black\", linetype = \"solid\", linewidth = 0.5\n  ) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nRate of EV share increase\nWe can visualize the share of total NMVR that are electric as it changes over time. We can also calculate and plot the rate of share increase.\n\nCode\ntheme_set(theme)\n\n# Create a line plot using the nmvr_quarterly_share dataset\nnmvr_quarterly_share_can |&gt;\n  ggplot() +\n\n  # adding points to highlight the max values\n  geom_point(\n    data = nmvr_quarterly_share_can,\n    aes(\n      x = ref_date, y = ifelse(share == max(share), share, NA),\n      color = \"Canada\"\n    ), na.rm = T, show.legend = FALSE\n  ) +\n  geom_point(\n    data = nmvr_quarterly_share_bc,\n    aes(\n      x = ref_date, y = ifelse(share == max(share), share, NA),\n      color = \"BC\"\n    ), na.rm = T, show.legend = FALSE\n  ) +\n\n  # Format the y-axis to display percentages with 1% accuracy\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  \n  # Set custom x-axis scale for dates\n  scale_x_date(\n    breaks = as.Date(c(\"2018-01-01\", \"2020-01-01\", \"2022-01-01\", \"2024-01-01\")), # Set specific break dates\n    labels = c(\"Q1 2018\", \"Q1 2020\", \"Q1 2022\", \"Q1 2024\") # Custom labels for each break\n  ) +\n\n  # Add lines for Canada and BC share growth\n  geom_line(aes(x = ref_date, y = share, color = \"Canada\"),\n    linewidth = 1, show.legend = TRUE\n  ) +\n  geom_line(\n    data = nmvr_quarterly_share_bc,\n    aes(\n      x = nmvr_quarterly_share_bc$ref_date,\n      y = nmvr_quarterly_share_bc$share, color = \"BC\"\n    ),\n    linewidth = 1, show.legend = TRUE\n  ) +\n\n  # adding text labels for the max values\n  geom_text(\n    data = nmvr_quarterly_share_bc, aes(\n      x = ref_date, y = share,\n      label = ifelse(share == max(share),\n        str_c(round(share * 100, 2), \"%\"),\n        \"\"\n      )\n    ),\n    nudge_y = 0.007, size = 3, show.legend = FALSE\n  ) +\n  geom_text(\n    data = nmvr_quarterly_share_can, aes(\n      x = ref_date, y = share,\n      label = ifelse(share == max(share),\n        str_c(round(share * 100, 2), \"%\"),\n        \"\"\n      )\n    ),\n    nudge_y = 0.007, size = 3, show.legend = FALSE\n  ) +\n\n  # Manually set the color scale and provide labels for the legend\n  scale_colour_manual(\n    values = c(\"BC\" = clr_plt[1], \"Canada\" = clr_plt[5]),\n    labels = c(\"BC\", \"Canada\")\n  ) +\n  labs(\n    title = \"Share of new registrations that are electric, Canada vs BC, Q1 2018 - Q2 2024\",\n    subtitle = \"Includes fully battery-electric and plug-in hybrids.\",\n    caption = \"Data source: Statistics Canada. Tables 20-10-0021-01 and 20-10-0024-01, New motor vehicle registrations\"\n  ) +\n  ylab(label = \"% of Total NMVR\") +\n\n  # adjust theme\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(\n      size = 10, angle = 90,\n      margin = margin(r = 10, unit = \"pt\"),\n      vjust = 3, hjust = 0.5\n    ),\n    plot.title = element_text(vjust = 0.8),\n    legend.title = element_blank(),\n    legend.position = \"inside\",\n    legend.key.size = unit(4, \"mm\"),\n    legend.justification.inside = c(0, 1),\n    legend.text.position = \"left\",\n    legend.direction = \"horizontal\",\n    legend.box.margin = margin(t = 0, b = 0, unit = \"pt\"),\n  )\n# share rate change timeseries just for Canada\nnmvr_share_rate |&gt;\n  ggplot(aes(x = year, y = share_growth)) +\n  geom_line(na.rm = T, linewidth = 1, colour = clr_plt[5]) +\n\n  # add smoothing to make trend more visible\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", color = clr_plt[4], linewidth = 0.5, fill = clr_plt[3], na.rm = T) +\n\n  # make sure labels are in percent\n  scale_y_continuous(labels = scales::label_percent(scale = 1)) +\n  labs(\n    title = \"Rate of EV share change, YoY, Canada, 2012 - 2023\",\n    subtitle = \"Includes fully battery-electric and plug-in hybrids.\",\n    caption = \"Data source: Statistics Canada. Tables 20-10-0021-01 and 20-10-0024-01, New motor vehicle registrations\"\n  ) +\n  ylab(label = \"% Rate Change of EV Share\") +\n\n  # adjust theme\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(\n      size = 10, angle = 90,\n      margin = margin(r = 10, unit = \"pt\"),\n      vjust = 3, hjust = 0.5\n    ),\n    plot.title = element_text(vjust = 0.8)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal data\nHow does Canada’s EV share growth compare to the top 5 countries with the highest EV share of total vehicle sales?\n\n\nCode\ntheme_set(theme)\n\nev_share_global_fill |&gt;\n  ggplot(aes(x = year, y = ev_share)) +\n  geom_col(aes(fill = is_canada), show.legend = FALSE) +\n  scale_fill_manual(values = c(\"TRUE\" = clr_plt[5], \"FALSE\" = clr_plt[3])) +\n  scale_y_continuous(labels = scales::percent) +\n\n  # adding labels of the maximum values with arrows\n  geom_segment(aes(x = 2020, xend = 2023, y = share),\n    data = max_share, arrow = arrow(length = unit(0.1, \"cm\")),\n    color = \"black\", linetype = \"solid\", linewidth = 0.5\n  ) +\n  geom_text(aes(x = 2020, y = share, label = scales::percent(share)),\n    data = max_share,\n    hjust = 1.1, color = \"grey30\", size = 3\n  ) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Share of new cars sold that are electric (Top 5 + Canada), 2010 to 2023\",\n    subtitle = \"Includes fully battery-electric and plug-in hybrids.\",\n    caption = \"source: International Energy Agency. Global EV Outlook 2024. – processed by Our World in Data\"\n  ) +\n\n  # facet wrap showing the x axis labels for each plot with all_x\n  facet_wrap(~entity, axes = \"all_x\")\n\n\n\n\n\n\n\n\n\n\n\nEstimating when EVs will dominate the market\nWe can do an estimation of when the majority of cars on the road will be zero emission vehicles by using the summary function to find the distribution of the EV share rate. Then by using the mean and the min values, calculate a range of years representing “best-case” and “minimum” number of years.\nTo achieve this we will use an exponential growth formula: \\[x(t) = x_0(1 + r)^t\\]\nUsing the mean rate of growth, 0.53 and the current share of NMVR that are electric, 0.11, the equation evaluates to:\n\\[ t = \\frac{log(1 / 0.11)}{log(1 + 0.49)}\\]\nThe following code will find the range of years using the mean and min rate of growth.\n\n\nCode\n# filter outlier\nnmvr_share_rate &lt;- nmvr_share_rate |&gt;\n  filter(year &gt; 2012)\n\nsummary(nmvr_share_rate$share_growth)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.87   32.27   50.64   53.36   59.69  132.04 \n\n\nWe will define a function to calculate exponential growth:\n\n\nCode\n# create a function to find when at the current mean and min rate of share growth\n# 100% of NMVR will be EV\nev_saturation_func &lt;- function(df, share, year, rate) {\n  # Calculate the average growth rate\n  average_growth &lt;- mean(rate[!is.na(rate)]) / 100 # Convert percentage to a decimal\n\n  # using the min rate\n  min_growth &lt;- min(rate, na.rm = TRUE) / 100\n\n  # Use the current share and growth rate to predict when it reaches 100%\n  current_year &lt;- max(year)\n  current_share &lt;- tail(share, 1)\n\n  # Solve for t when share reaches 100% using exponential growth formula\n  t &lt;- log(1 / current_share) / log(1 + average_growth)\n  v &lt;- log(1 / current_share) / log(1 + min_growth)\n\n  # Calculate the future year\n  future_year &lt;- current_year + t\n  future_year2 &lt;- current_year + v\n  str_c(\n    \"With a constant rate of \", round(min_growth * 100, 1),\n    \"% - \", round(average_growth * 100, 1),\n    \"%, the share of total new vehicles that are electric will reach 100% by \",\n    round(future_year), \"-\", round(future_year2)\n  )\n}\n\n\n\n\nCode\n# run function\nev_saturation_func(\n  nmvr_share_rate,\n  nmvr_share_rate$share,\n  nmvr_share_rate$year,\n  nmvr_share_rate$share_growth\n)\n\n\n[1] \"With a constant rate of 20.9% - 53.4%, the share of total new vehicles that are electric will reach 100% by 2028-2035\"\n\n\n\n\nCode\nsummary(global_share_growth$share_growth)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  8.333  51.852  61.538  97.455  71.429 518.182       1 \n\n\n\n\nCode\n# run function\nev_saturation_func(\n  global_share_growth,\n  global_share_growth$ev_yearly_share,\n  global_share_growth$year,\n  global_share_growth$share_growth\n)\n\n\n[1] \"With a constant rate of 8.3% - 97.5%, the share of total new vehicles that are electric will reach 100% by 2026-2044\""
  },
  {
    "objectID": "posts/ev-sales/index.html#key-findings",
    "href": "posts/ev-sales/index.html#key-findings",
    "title": "Canada’s Path to an EV Future",
    "section": "Key findings",
    "text": "Key findings\nIn BC the share of registrations that were EV was 20.7% in 2023, compared to around 13% in all of Canada. The share of new vehicle registrations that are electric in Canada is rapidly growing, averaging 53% per year.\nAssuming the rate of growth remains steady, new vehicle registrations in Canada will be predominately zero emission within the next 4-11 years. Although it is unlikely the rate will stay constant, nor is it likely everyone will want to drive an EV, it is evident that Canada is shifting rapidly towards an EV future.\nGlobally, Norway is leading the way with 93% of 2023’s car sales being EV. Iceland follows closely at 71%. The share rate of growth of total vehicle sales that are EV has a much broader distribution range from a minimum of 8.3% to the mean of 97.5%. If those rates are maintained, global new vehicle registrations could be predominately zero emission within the next 2-20 years."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nTransforming GA4 Data into Actionable Insights with SQL + Power BI\n\n\n\nSQL\n\n\nPowerBi\n\n\n\n\n\n\n\nSkylar Carroll\n\n\nMar 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoblaw Sales Analysis\n\n\n\nTableau\n\n\nExcel\n\n\n\n\n\n\n\nSkylar Carroll, Stuart N, Kate G, Jess L\n\n\nDec 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanada’s Path to an EV Future\n\n\n\nR\n\n\n\n\n\n\n\nSkylar Carroll\n\n\nOct 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVegitation Density in the Greater Vancouver Area\n\n\n\npython\n\n\npowerbi\n\n\n\n\n\n\n\nSkylar Carroll\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefi Research Dashboard\n\n\n\npowerbi\n\n\n\n\n\n\n\nSkylar Carroll\n\n\nApr 6, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Skylar Carroll",
    "section": "",
    "text": "In 2021, Skylar transitioned from a career as a cook to trading commodities full-time, leveraging a custom-built trading program to automate trade execution and analyze performance leading to an increase in annual net profit by 15%. This work sparked a passion for data analysis, leading him to pursue advanced skills in business intelligence.\nSkylar is excited by building new tools and automating workflows to help businesses and organizations save time and money. He is Microsoft Certified in Power BI and has obtained the Business Intelligence and Data Analytics Micro-certificate from the University of Victoria."
  },
  {
    "objectID": "posts/defi-llama/index.html",
    "href": "posts/defi-llama/index.html",
    "title": "Defi Research Dashboard",
    "section": "",
    "text": "Defi Llama\nDefi Llama is a decentralized finance blockchain research and tracking website with a public REST API. I designed a simple interface to quickly find new or fast moving defi projects, filtered by type or chain, with search options to find specific protocols.\nI used PowerBI to connect to web sources. I transformed the json data in powerquery and removed the data I wasn’t interested in. After building a few table relationships and measure, I designed and built a dashboard.\n navigate to dashboard"
  },
  {
    "objectID": "posts/loblaw-analysis/index.html",
    "href": "posts/loblaw-analysis/index.html",
    "title": "Loblaw Sales Analysis",
    "section": "",
    "text": "Introduction\nA team based decision support and BI project for UVic’s BIDA 301: Business Intelligence and Data Analytics Fundamentals course. Students were tasked with planning the analysis of a real-world dataset. Deliverables of the project were completed in three sprints over the course of 3 weeks:\n\na proposal\nan execution plan and\na comprehensive visualization, project report, and a PowerPoint presentation to be presented by all team members to the class.\n\nDownload the PowerPoint files here. Explore the Tableau here.\n\n\nTools and Methods\nPowerQuery for data extraction - Excel for linear regression and Holt-Winter analysis - Tableau for visualization - Power Point for presentation slides.\n\n\nProblem Statement\nHow do Food Consumer Price Index (CPI) fluctuations, global supply chain disruptions, and real disposable household income changes influence Loblaw’s food sales performance?\n\n\nThe Data\nWe used PowerQuery to extract tables from 10 years of Loblaw annual financial reports https://www.loblaw.ca/en/investors-reports/ (2022-2024) https://www.sedarplus.ca (pre-2022)\nConsumer price index (CPI) - Consumer Price Index (all-items) measures the average change over time in the prices paid by consumers for a wide range of goods and services, including food, housing, transportation, and medical care. https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810025601\nReal household disposable income (RHDI) - the income available to households after taxes, adjusted for inflation. It represents the amount of money that households have available to spend or save, considering changes in price levels. https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=3610010501\nGlobal supply chain pressure index (GSCPI) - a composite index that tracks global supply chain disruptions. It combines several factors such as transportation costs, delivery times, and supply shortages to measure the overall stress on global supply chains. https://www.statista.com/statistics/1315308/global-supply-chain-pressure-index/\n\n\nAnalytic Approach\n\nLinear Regression Analysis\n\nTo determine whether any of the three independent variables (CPI, RHDI and GSCPI) correlate with Loblaw’s sales, regression analysis is done using the 4-quarter sales moving average to adjust the sales data with seasonality (as shown above). Using the smoothed series, Loblaw’s sales resulted in a strong correlation (R square = 88%) between Food CPI and RHDI, both variables having P-values less than 0.05. Global supply chain pressure index (GSCPI) has a P-value higher than 0.05, which means it is not a significant predictor of Sales, so we removed the GSCPI variable to create a more accurate model. In the regression output above, all the P-values are now showing below 0.05, while the R square value also remains at 0.8754, a strong fit of correlation, which means 88% of observations in Loblaw’s smoothed Sales series are explained by Food CPI and RHDI.\n\n\nSales Forecasting\n \nUsing Holt-Winter’s method, optimal values for alpha, beta and gamma are found and a 12-month forecast for sales is created. This will be used to optimize Loblaw’s inventory levels and drive promotion strategies to anticipate demand on the forecast sales. Even so, with heteroskedasticity in the scatter plots, the model would not be able to fully capture the data dynamics. Thus, qualitative factors must be considered, understanding the business issue to look at retail industry trends, competition, regulatory policies and consumer behavior that may help manage sales forecasting moving forward.\n\n\nLimitations:\nThe financial performance of one grocery conglomerate (Loblaw) is not indicative of the performance of the grocery retail market overall.\nGlobal supply chain disruption indicators may not fully capture the complexities of Loblaw’s operations and localized issues in Canada.\nCOVID was not accounted for as an outlier in the Holt-Winter analysis.\n\n\n\nKey Findings\nClick link to Tableau dashboard\n\n2015–2019:\n\nSteady CPI Growth: Yearly % change fluctuated between 1.9% to 3.9%, showing moderate growth. \n\n\n\n\n2020–2023:\n\nSupply Chain Issues: Highlighted period of disruption, correlating with significant increases in food CPI:\n\n2022: 8.5% growth in food CPI.\n2023: 8.0% growth in food CPI.\n\nThis sharp increase aligns with pandemic impacts, labor shortages, and globalsupply chain issues.\n\n2024:\n\nCPI Growth Moderation: The % change drops to 3.0%, suggesting easing supply chain pressures and inflation rates.\n\n\nThe analysis reveals a complex interplay between consumer behavior and macroeconomic trends. Consumers are rapidly losing buying power as food prices rise faster than other goods, with the Food Consumer Price Index (CPI) outpacing overall inflation. Despite these challenges, Loblaw has benefited from increased prices, capitalizing on food inflation and shifts in consumer behavior during the COVID-19 pandemic. Retail food sales tripled during this period, driven by increased Real Household Disposable Income (RHDI) and a surge in grocery demand as households adjusted to lockdowns. In 2023, retail food sales continued to grow, largely due to a spike in food inflation, emphasizing how rising costs disproportionately impact consumers while bolstering revenue for grocery retailers."
  },
  {
    "objectID": "posts/ga4/index.html",
    "href": "posts/ga4/index.html",
    "title": "Transforming GA4 Data into Actionable Insights with SQL + Power BI",
    "section": "",
    "text": "This project began with a question, “How can we flatten and extract minimal GA4 data to produce a light weight semantic model in Power BI?” GA4’s event-based data can result in hundreds of thousands of rows of data per month and due to the nested STRUCT and ARRAY data types, flattening event data multiplies the number of rows for any given user session. It becomes impractical and unnecessary to extract every single field for an effective, high-level Power BI dashboard."
  },
  {
    "objectID": "posts/ga4/index.html#quarto",
    "href": "posts/ga4/index.html#quarto",
    "title": "How to flatten Google Analytics 4 data structures in BigQuery",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/ga4/index.html#running-code",
    "href": "posts/ga4/index.html#running-code",
    "title": "How to flatten Google Analytics 4 data structures in BigQuery",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n-- Define a common table expression (CTE) to extract session-level data from Google Analytics 4 (GA4) event logs\nWITH session_level AS (\n  SELECT\n    event_date, -- The date of the event\n    extract(\n      datetime\n      FROM\n        timestamp_micros(event_timestamp)\n    ) AS event_timestamp, -- Convert event timestamp to a readable datetime format\n    user_pseudo_id,\n    \n    -- Create a unique join key for session-level analysis by combining user ID, session ID, timestamp, and event name\n    concat(\n      user_pseudo_id,\n      (\n        SELECT\n          value.int_value\n        FROM\n          unnest(event_params)\n        WHERE\n          KEY = 'ga_session_id'\n      ),\n      event_timestamp,\n      event_name\n    ) AS join_key,\n    \n    -- Create a session key using user ID and session ID\n    concat(\n      user_pseudo_id,\n      (\n        SELECT\n          value.int_value\n        FROM\n          unnest(event_params)\n        WHERE\n          KEY = 'ga_session_id'\n      )\n    ) AS session_key,\n    event_name,\n    \n    -- Extract the page title from event parameters\n    (\n      SELECT\n        value.string_value\n      FROM\n        unnest(event_params)\n      WHERE\n        KEY = 'page_title'\n    ) AS page_title,\n    \n    -- Extract the page URL from event parameters\n    (\n      SELECT\n        value.string_value\n      FROM\n        unnest(event_params)\n      WHERE\n        KEY = 'page_location'\n    ) AS page_location,\n    \n    -- Extract engagement time (in milliseconds) from event parameters\n    (\n      SELECT\n        value.int_value\n      FROM\n        unnest(event_params)\n      WHERE\n        KEY = 'engagement_time_msec'\n    ) AS event_engagement_time_msec,\n    \n    -- Extract traffic source information (where the user came from)\n    traffic_source.name AS traffic_source_name,\n    traffic_source.medium AS traffic_source_medium,\n    traffic_source.source AS traffic_source_source,\n    device.category AS device\n  FROM\n    -- change this to your google analytics 4 export location in bigquery\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _table_suffix BETWEEN '20201231'\n    AND '20210131'\n),\n\n-- Define another CTE to clean and standardize page titles\npage_clean AS (\n  SELECT\n    *\n  EXCEPT\n    (page_title),\n    \n    -- Clean the page_title field\n    CASE WHEN page_title IS NULL THEN REPLACE(\n      REGEXP_EXTRACT(\n        REGEXP_REPLACE(trim(page_location), r '/$', ''),\n        r '[^/]+$'\n      ),\n      r '[+-]',\n      ' '\n    ) \n    -- Extract last part of URL as a fallback page title, replacing special characters\n    WHEN trim(page_location) = trim(\"https://shop.googlemerchandisestore.com/New/\") THEN \"New\" WHEN trim(page_location) = trim(\n      \"https://shop.googlemerchandisestore.com/details/New/New/GGL1720_Google Notabag - Blue\"\n    ) THEN \"Google Notabag - Blue\" WHEN trim(page_location) = trim(\n      \"https://shop.googlemerchandisestore.com/details/New/New/GGL1720_Google\"\n    ) THEN \"GGL1720_Google\" WHEN trim(page_location) = trim(\n      \"https://shop.googlemerchandisestore.com/google+redesign/apparel/google+kirkland+campus+unisex+tee\"\n    ) THEN \"google+kirkland+campus+unisex+tee\" ELSE page_title -- Keep the original page title if available\n    END AS page_title_cleaned\n  FROM\n    session_level\n)\nSELECT\n  *\nFROM\n  page_clean\nYou can add options to executable code like this\n-- Aggregate user-level data from Google Analytics 4 (GA4) event logs\nSELECT\n  user_pseudo_id, -- Anonymous user identifier\n  \n  -- Calculate the average lifetime value (LTV) revenue per user\n  AVG(user_ltv.revenue) AS ltv,\n  \n  -- Retrieve the latest currency associated with the user's LTV\n  MAX(user_ltv.currency) AS ltv_currency,\n  \n  -- Get the most recent country associated with the user\n  MAX(geo.country) AS country,\n  \n  -- Get the most recent region (state/province) associated with the user\n  MAX(geo.region) AS region\nFROM\n  -- Replace this with your GA4 BigQuery export table\n  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nWHERE\n  -- Filter for events within the specified date range\n  _table_suffix BETWEEN '20201231'\n  AND '20210131'\nGROUP BY\n  user_pseudo_id; -- Aggregate data at the user level\n\n-- Extract and aggregate item details from Google Analytics 4 (GA4) event logs\nSELECT\n  i.item_id, -- Unique identifier for the item\n  i.item_name, -- Name of the item\n  \n  -- Retrieve the most recent category associated with the item\n  MAX(i.item_category) AS category\nFROM\n  -- Replace this with your GA4 BigQuery export table\n  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n  \n  -- Expand the nested 'items' array to extract individual item details\n  UNNEST(items) AS i\nGROUP BY\n  i.item_id, -- Group by item ID to aggregate item-level data\n  i.item_name; -- Group by item name to maintain unique item records\n--purchase details\n\nSELECT concat(user_pseudo_id,\n                (SELECT value.int_value\n                 FROM unnest(event_params)\n                 WHERE KEY = 'ga_session_id'),event_timestamp, event_name) AS join_key,\n       ecommerce.transaction_id,\n       ecommerce.total_item_quantity,\n       ecommerce.purchase_revenue_in_usd,\n       ecommerce.purchase_revenue,\n       ecommerce.refund_value_in_usd,\n       ecommerce.refund_value,\n       ecommerce.shipping_value_in_usd,\n       ecommerce.shipping_value,\n       ecommerce.tax_value_in_usd,\n       ecommerce.tax_value,\n       ecommerce.unique_items\nFROM -- change this to your google analytics 4 export location in bigquery\n `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nWHERE event_name = 'purchase'\n  AND _table_suffix BETWEEN '20201231' AND '20210131'\n--purchase details\n\nSELECT event_timestamp,\n       concat(user_pseudo_id,\n                (SELECT value.int_value\n                 FROM unnest(event_params)\n                 WHERE KEY = 'ga_session_id'),event_timestamp, event_name) AS join_key,\n       i.item_id,\n       i.item_name,\n       i.item_brand,\n       i.item_variant,\n       i.item_category,\n       i.coupon,\n       i.affiliation,\n       i.location_id,\n       i.item_list_id,\n       i.item_list_name,\n       i.item_list_index,\n       i.promotion_id,\n       i.promotion_name,\n       i.creative_name,\n       i.creative_slot,\n       i.price_in_usd,\n       i.price,\n       i.quantity,\n       i.item_revenue_in_usd,\n       i.item_revenue,\n       i.item_refund_in_usd\nFROM -- change this to your google analytics 4 export location in bigquery\n `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nLEFT JOIN unnest(items) i\nWHERE event_name = 'purchase'\n  AND _table_suffix BETWEEN '20201231' AND '20210131'\n--sessions\nWITH prep AS\n  (SELECT event_timestamp,\n          concat(user_pseudo_id,\n                   (SELECT value.int_value\n                    FROM unnest(event_params)\n                    WHERE KEY = 'ga_session_id')) AS session_key,\n          coalesce(cast(\n                          (SELECT value.string_value\n                           FROM unnest(event_params)\n                           WHERE KEY = 'session_engaged') AS int), 0) AS session_engaged,\n          count(DISTINCT\n                  (SELECT value.string_value\n                   FROM unnest(event_params)\n                   WHERE KEY = 'page_location')) over(PARTITION BY user_pseudo_id,\n                                                        (SELECT value.int_value\n                                                         FROM unnest(event_params)\n                                                         WHERE KEY = 'ga_session_id')) AS unique_page_views,\n\n     (SELECT value.int_value\n      FROM unnest(event_params)\n      WHERE KEY = 'engagement_time_msec') AS session_engagement_time_msec\n   FROM -- change this to your google analytics 4 export location in bigquery\n `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n   WHERE _table_suffix BETWEEN '20201231' AND '20210131')\nSELECT max(event_timestamp) - min(event_timestamp) AS session_duration_msec,\n       session_key,\n       max(session_engaged) AS session_engaged,\n       max(unique_page_views) AS upv,\n       sum(session_engagement_time_msec) AS session_engagement_time_msec\nFROM prep\nGROUP BY session_key\n\n\n\nSemantic Model"
  },
  {
    "objectID": "posts/ga4/index.html#key-challenges-and-solutions",
    "href": "posts/ga4/index.html#key-challenges-and-solutions",
    "title": "Transforming GA4 Data into Actionable Insights with SQL + Power BI",
    "section": "Key Challenges and Solutions",
    "text": "Key Challenges and Solutions\n\nComplex Data Structures: While GA4’s use of ‘STRUCT’ and ‘ARRAY’ nested data types enable efficient storage of complex, hierarchical user interactions, they do require careful handling for querying and extraction.\nData Selection: A list of all required attributes should be created before beginning to write the SQL. It is critical to understand the definitions of each attribute and what they represent. GA4 documentation should be referenced carefully, and audits be conducted before launching any live service dashboard.\nData Extraction: The SQL queries utilize UNNEST, LEFT JOIN and subqueries to flatten ‘ARRAY’ and ‘STRUCT’ data. This multiplies the number of rows per session and GA4’s new event-based model means large amounts of traffic may create thousands of rows in the database per second. For a high-level dashboard to operate smoothly in Power BI, data needs to be selected strategically.\nData Transformation: It’s best practice to transform and clean your data in SQL as much as possible to create meaningful insights, such as cleaning page titles and calculating session engagement metrics. Query folding in Power Query doesn’t always work and transformations can create performance bottlenecks. Let the source do the heavy lifting.\nSemantic Modeling: The goal is to create a simplified semantic model suitable for Power BI analysis, focusing on user sessions, purchase details, and item information. Parameters as start and end dates allow for easy updates."
  },
  {
    "objectID": "posts/ga4/index.html#sql-queries-explained",
    "href": "posts/ga4/index.html#sql-queries-explained",
    "title": "Transforming GA4 Data into Actionable Insights with SQL + Power BI",
    "section": "SQL Queries Explained",
    "text": "SQL Queries Explained\nHere’s a breakdown of the SQL queries:\n\n1. Session-Level Data Extraction\n\nThis query extracts session-level information, including timestamps, user IDs, page views, and traffic sources. It also creates unique keys for session analysis and cleans page titles for better readability.\n\n\n-- Define a CTE to extract session-level data from GA4 event logs\nWITH session_level AS (\n  SELECT\n    event_date, -- The date of the event\n    extract(\n      datetime\n      FROM\n        timestamp_micros(event_timestamp)\n    ) AS event_timestamp, -- Convert event timestamp to a readable datetime format\n    user_pseudo_id,\n    \n    -- Create a unique join key for session-level analysis by combining user ID, session ID, timestamp, and event name\n    concat(\n      user_pseudo_id,\n      (\n        SELECT\n          value.int_value\n        FROM\n          unnest(event_params)\n        WHERE\n          KEY = 'ga_session_id'\n      ),\n      event_timestamp,\n      event_name\n    ) AS join_key,\n    \n    -- Create a session key using user ID and session ID\n    concat(\n      user_pseudo_id,\n      (\n        SELECT\n          value.int_value\n        FROM\n          unnest(event_params)\n        WHERE\n          KEY = 'ga_session_id'\n      )\n    ) AS session_key,\n    event_name,\n    \n    -- Extract the page title from event parameters\n    (\n      SELECT\n        value.string_value\n      FROM\n        unnest(event_params)\n      WHERE\n        KEY = 'page_title'\n    ) AS page_title,\n    \n    -- Extract the page URL from event parameters\n    (\n      SELECT\n        value.string_value\n      FROM\n        unnest(event_params)\n      WHERE\n        KEY = 'page_location'\n    ) AS page_location,\n    \n    -- Extract engagement time (in milliseconds) from event parameters\n    (\n      SELECT\n        value.int_value\n      FROM\n        unnest(event_params)\n      WHERE\n        KEY = 'engagement_time_msec'\n    ) AS event_engagement_time_msec,\n    \n    -- Extract traffic source information (where the user came from)\n    traffic_source.name AS traffic_source_name,\n    traffic_source.medium AS traffic_source_medium,\n    traffic_source.source AS traffic_source_source,\n    device.category AS device\n  FROM\n    -- change this to your google analytics 4 export location in bigquery\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _table_suffix BETWEEN '20201231' -- Change the dates to your parameters in Power Query M code\n    AND '20210131'\n),\n\n-- Define another CTE to clean and standardize page titles\npage_clean AS (\n  SELECT\n    *\n  EXCEPT\n    (page_title),\n    \n    -- Clean the page_title field\n    CASE WHEN page_title IS NULL THEN REPLACE(\n      REGEXP_EXTRACT(\n        REGEXP_REPLACE(trim(page_location), r '/$', ''),\n        r '[^/]+$'\n      ),\n      r '[+-]',\n      ' '\n    ) \n    -- Extract last part of URL as a fallback page title, replacing special characters\n    WHEN trim(page_location) = trim(\"https://shop.googlemerchandisestore.com/New/\") THEN \"New\" WHEN trim(page_location) = trim(\n      \"https://shop.googlemerchandisestore.com/details/New/New/GGL1720_Google Notabag - Blue\"\n    ) THEN \"Google Notabag - Blue\" WHEN trim(page_location) = trim(\n      \"https://shop.googlemerchandisestore.com/details/New/New/GGL1720_Google\"\n    ) THEN \"GGL1720_Google\" WHEN trim(page_location) = trim(\n      \"https://shop.googlemerchandisestore.com/google+redesign/apparel/google+kirkland+campus+unisex+tee\"\n    ) THEN \"google+kirkland+campus+unisex+tee\" ELSE page_title -- Keep the original page title if available\n    END AS page_title_cleaned\n  FROM\n    session_level\n)\nSELECT\n  *\nFROM\n  page_clean\n\n\n2. User-Level Aggregation\n\nThis query aggregates user-level data, such as lifetime value (LTV), country, and region.\n\n-- Aggregate user-level data\nSELECT\n  user_pseudo_id, -- Anonymous user identifier\n  \n  -- Calculate the average lifetime value (LTV) revenue per user\n  AVG(user_ltv.revenue) AS ltv,\n  \n  -- Retrieve the latest currency associated with the user's LTV\n  MAX(user_ltv.currency) AS ltv_currency,\n  \n  -- Get the most recent country associated with the user\n  MAX(geo.country) AS country,\n  \n  -- Get the most recent region (state/province) associated with the user\n  MAX(geo.region) AS region\nFROM\n  -- Replace this with your GA4 BigQuery export table\n  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nWHERE\n  -- Filter for events within the specified date range\n  _table_suffix BETWEEN '20201231' -- Change the dates to your parameters in Power Query M code\n  AND '20210131'\nGROUP BY\n  user_pseudo_id; -- Aggregate data at the user level\n\n\n3. Item Details\n\nThis query retrieves item-related information.\n\n-- Extract and aggregate item details\nSELECT\n  i.item_id, -- Unique identifier for the item\n  i.item_name, -- Name of the item\n  \n  -- Retrieve the most recent category associated with the item\n  MAX(i.item_category) AS category\nFROM\n  -- Replace this with your GA4 BigQuery export table\n  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n  \n  -- Expand the nested 'items' array to extract individual item details\n  UNNEST(items) AS i\nGROUP BY\n  i.item_id, -- Group by item ID to aggregate item-level data\n  i.item_name; -- Group by item name to maintain unique item records\n\n\n4. Purchase - event level\n\nThis query extracts details of each purchase event.\n\n--purchase\n\nSELECT concat(user_pseudo_id,\n                (SELECT value.int_value\n                 FROM unnest(event_params)\n                 WHERE KEY = 'ga_session_id'),event_timestamp, event_name) AS join_key,\n       ecommerce.transaction_id,\n       ecommerce.total_item_quantity,\n       ecommerce.purchase_revenue_in_usd,\n       ecommerce.purchase_revenue,\n       ecommerce.refund_value_in_usd,\n       ecommerce.refund_value,\n       ecommerce.shipping_value_in_usd,\n       ecommerce.shipping_value,\n       ecommerce.tax_value_in_usd,\n       ecommerce.tax_value,\n       ecommerce.unique_items\nFROM -- change this to your google analytics 4 export location in bigquery\n `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nWHERE event_name = 'purchase'\n  AND _table_suffix BETWEEN '20201231' AND '20210131' -- Change the dates to your parameters in Power Query M code\n\n\n5. Purchase Details - Item level\n\nThis query extracts detailed information about purchased items, including item IDs, names, prices, and quantities.\n\n--purchase details\n\nSELECT event_timestamp,\n       concat(user_pseudo_id,\n                (SELECT value.int_value\n                 FROM unnest(event_params)\n                 WHERE KEY = 'ga_session_id'),event_timestamp, event_name) AS join_key,\n       i.item_id,\n       i.item_name,\n       i.item_brand,\n       i.item_variant,\n       i.item_category,\n       i.coupon,\n       i.affiliation,\n       i.location_id,\n       i.item_list_id,\n       i.item_list_name,\n       i.item_list_index,\n       i.promotion_id,\n       i.promotion_name,\n       i.creative_name,\n       i.creative_slot,\n       i.price_in_usd,\n       i.price,\n       i.quantity,\n       i.item_revenue_in_usd,\n       i.item_revenue,\n       i.item_refund_in_usd\nFROM -- change this to your google analytics 4 export location in bigquery\n `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nLEFT JOIN unnest(items) i\nWHERE event_name = 'purchase'\n  AND _table_suffix BETWEEN '20201231' AND '20210131' -- Change the dates to your parameters in Power Query M code\n\n\n6. Session Analysis\n\nThis query calculates session metrics like session duration, engagement, and unique page views.\n\n--sessions\nWITH prep AS\n  (SELECT event_timestamp,\n          concat(user_pseudo_id,\n                   (SELECT value.int_value\n                    FROM unnest(event_params)\n                    WHERE KEY = 'ga_session_id')) AS session_key,\n          coalesce(cast(\n                          (SELECT value.string_value\n                           FROM unnest(event_params)\n                           WHERE KEY = 'session_engaged') AS int), 0) AS session_engaged,\n          count(DISTINCT\n                  (SELECT value.string_value\n                   FROM unnest(event_params)\n                   WHERE KEY = 'page_location')) over(PARTITION BY user_pseudo_id,\n                                                        (SELECT value.int_value\n                                                         FROM unnest(event_params)\n                                                         WHERE KEY = 'ga_session_id')) AS unique_page_views,\n\n     (SELECT value.int_value\n      FROM unnest(event_params)\n      WHERE KEY = 'engagement_time_msec') AS session_engagement_time_msec\n   FROM -- change this to your google analytics 4 export location in bigquery\n `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n   WHERE _table_suffix BETWEEN '20201231' AND '20210131')\nSELECT max(event_timestamp) - min(event_timestamp) AS session_duration_msec,\n       session_key,\n       max(session_engaged) AS session_engaged,\n       max(unique_page_views) AS upv,\n       sum(session_engagement_time_msec) AS session_engagement_time_msec\nFROM prep\nGROUP BY session_key"
  },
  {
    "objectID": "posts/ga4/index.html#semantic-model",
    "href": "posts/ga4/index.html#semantic-model",
    "title": "Transforming GA4 Data into Actionable Insights with SQL + Power BI",
    "section": "Semantic Model",
    "text": "Semantic Model\nThe extracted and transformed data can be used to build a semantic model in Power BI, enabling interactive analysis and reporting.\n\n\n\nSemantic Model"
  }
]